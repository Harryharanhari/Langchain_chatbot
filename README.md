# ğŸ“„ PDF Context Chatbot (LangChain + Ollama + Streamlit)

A lightweight **PDF-based chatbot** that extracts content from a PDF, stores it as structured text (JSON-like chunks), and allows users to ask questions based strictly on the document context.

Built using:

* **LangChain**
* **Ollama (Local LLMs)**
* **Streamlit**
* **PyPDF**

---

# ğŸš€ Features

âœ… Upload any PDF
âœ… Automatic text extraction
âœ… Smart text chunking
âœ… Context-based Q&A
âœ… Local LLM (no API cost)
âœ… Simple Streamlit UI
âœ… Privacy-friendly (runs locally)

---

# ğŸ§  How It Works

This project follows a **basic Retrieval-Augmented Generation (RAG)** workflow.

### Step 1 â€” PDF Processing

* PDF is uploaded
* Text is extracted using PyPDFLoader
* Text is split into chunks

### Step 2 â€” JSON Knowledge Base

* Each chunk is stored as structured data
* Acts as a mini knowledge base

### Step 3 â€” Context Retrieval

* User question is matched with relevant chunks
* Keyword-based scoring finds top matches

### Step 4 â€” LLM Answering

* Relevant context + question sent to Ollama
* Model answers ONLY from provided context

---

# ğŸ› ï¸ Tech Stack

| Tool      | Purpose                |
| --------- | ---------------------- |
| LangChain | LLM pipeline framework |
| Ollama    | Run local LLMs         |
| Streamlit | Web UI                 |
| PyPDF     | PDF text extraction    |

---

# ğŸ“¦ Installation

## 1ï¸âƒ£ Clone Repo

```bash
git clone <your-repo-url>
cd <project-folder>
```

---

## 2ï¸âƒ£ Create Virtual Environment

```bash
python -m venv venv
```

Activate:

**Windows**

```bash
venv\Scripts\activate
```

**Mac/Linux**

```bash
source venv/bin/activate
```

---

## 3ï¸âƒ£ Install Dependencies

```bash
pip install -r requirements.txt
```

---

## 4ï¸âƒ£ Install Ollama

Download from:

[https://ollama.com](https://ollama.com)

---

## 5ï¸âƒ£ Pull LLM Model

Recommended:

```bash
ollama pull phi3
```

or

```bash
ollama pull llama3
```

---

# â–¶ï¸ Running the App

```bash
streamlit run streamlit_app.py
```

Browser will open automatically.

---

# ğŸ“Œ Usage

1. Upload a PDF
2. Click **Process PDF**
3. Ask questions related to the document
4. Get context-based answers

---

# âš ï¸ Requirements

* Python **3.10 or 3.11 recommended**
* Ollama running locally
* Minimum 8GB RAM (for larger models)

---

# ğŸ”® Future Improvements

* Embedding-based semantic search
* FAISS/Chroma vector DB
* Chat memory
* Multi-PDF support
* Cloud deployment
* Streaming responses

---

# ğŸ¯ Learning Outcomes

This project demonstrates:

* RAG architecture basics
* Document processing
* Context retrieval
* Local LLM integration
* Streamlit deployment

---

# ğŸ“œ License

MIT License
Free for learning and research.

---

# ğŸ™Œ Acknowledgements

* LangChain team
* Ollama
* Streamlit
* Open-source community

---

# ğŸ‘¨â€ğŸ’» Author

**Hari Haran**
AI & Data Science Student

Just say ğŸ‘
